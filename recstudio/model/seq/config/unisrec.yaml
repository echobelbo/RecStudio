model:
  train_stage: pretrain
  n_exps: 8
  temperature: 0.07


  plm_size: 768
  adaptor_dropout_prob: 0.2
  adaptor_layers: [768,300]
  embed_dim: 300

  lamda: 1e-3
  
  hidden_size: 300
  layer_num: 2
  head_num: 2
  dropout_rate: 0.5
  activation: 'gelu'
  layer_norm_eps: 1e-12

train:
  # negative_count: 1
  init_method: normal
  sampler: ~
  save_interval_epochs: 5
